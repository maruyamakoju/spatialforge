#!/usr/bin/env python3
"""Dataset preparation for RailScan defect detection model training.

Downloads and merges multiple rail defect datasets into a unified
YOLO-format dataset for fine-tuning YOLOv8.

Sources:
  1. Roboflow Universe — rail defect datasets (YOLO format, API download)
  2. RSDDs (GitHub) — rail surface defect dataset
  3. RTFD (GitHub) — railway track fault detection dataset

Usage:
    python scripts/prepare_dataset.py --output ./datasets/railscan

The output directory will contain:
    railscan/
    ├── data.yaml          # YOLO dataset config
    ├── train/
    │   ├── images/
    │   └── labels/
    ├── val/
    │   ├── images/
    │   └── labels/
    └── test/
        ├── images/
        └── labels/
"""

from __future__ import annotations

import argparse
import logging
import shutil
import zipfile
from pathlib import Path

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# ── Class mapping ─────────────────────────────────────────────
# All source datasets are remapped to these 11 classes.

CLASSES = [
    "rail_crack",          # 0
    "rail_wear",           # 1
    "rail_corrugation",    # 2
    "rail_spalling",       # 3
    "fastener_missing",    # 4
    "fastener_broken",     # 5
    "sleeper_crack",       # 6
    "sleeper_decay",       # 7
    "ballast_fouling",     # 8
    "joint_defect",        # 9
    "gauge_anomaly",       # 10
]


def create_dataset_yaml(output_dir: Path) -> Path:
    """Create YOLO data.yaml configuration file."""
    yaml_content = f"""# RailScan Defect Detection Dataset
# Auto-generated by prepare_dataset.py

path: {output_dir.resolve()}
train: train/images
val: val/images
test: test/images

nc: {len(CLASSES)}
names:
"""
    for i, name in enumerate(CLASSES):
        yaml_content += f"  {i}: {name}\n"

    yaml_path = output_dir / "data.yaml"
    yaml_path.write_text(yaml_content, encoding="utf-8")
    logger.info("Created %s", yaml_path)
    return yaml_path


def setup_directory_structure(output_dir: Path) -> None:
    """Create the YOLO directory structure."""
    for split in ("train", "val", "test"):
        (output_dir / split / "images").mkdir(parents=True, exist_ok=True)
        (output_dir / split / "labels").mkdir(parents=True, exist_ok=True)
    logger.info("Directory structure created at %s", output_dir)


def download_roboflow_dataset(
    workspace: str,
    project: str,
    version: int,
    api_key: str,
    output_dir: Path,
) -> Path | None:
    """Download a dataset from Roboflow Universe.

    Requires ROBOFLOW_API_KEY environment variable or --roboflow-key argument.
    """
    try:
        from roboflow import Roboflow
    except ImportError:
        logger.warning(
            "roboflow package not installed. Install with: pip install roboflow\n"
            "Skipping Roboflow dataset download."
        )
        return None

    logger.info("Downloading from Roboflow: %s/%s v%d", workspace, project, version)

    rf = Roboflow(api_key=api_key)
    proj = rf.workspace(workspace).project(project)
    dataset = proj.version(version).download(
        "yolov8",
        location=str(output_dir / "roboflow_tmp"),
    )
    return Path(dataset.location)


def merge_roboflow_data(
    roboflow_dir: Path,
    output_dir: Path,
    class_remap: dict[str, int] | None = None,
    prefix: str = "rf",
) -> int:
    """Merge downloaded Roboflow dataset into unified structure.

    Args:
        roboflow_dir: Path to downloaded Roboflow dataset.
        output_dir: Target unified dataset directory.
        class_remap: Mapping from source class names to target class IDs.
        prefix: File name prefix to avoid collisions.

    Returns:
        Number of images merged.
    """
    count = 0
    for split in ("train", "valid", "test"):
        target_split = "val" if split == "valid" else split
        src_images = roboflow_dir / split / "images"
        src_labels = roboflow_dir / split / "labels"

        if not src_images.exists():
            continue

        for img_path in src_images.iterdir():
            if img_path.suffix.lower() not in (".jpg", ".jpeg", ".png", ".bmp"):
                continue

            # Copy image
            new_name = f"{prefix}_{img_path.name}"
            dst_img = output_dir / target_split / "images" / new_name
            shutil.copy2(img_path, dst_img)

            # Copy/remap label
            label_path = src_labels / (img_path.stem + ".txt")
            if label_path.exists():
                dst_label = output_dir / target_split / "labels" / (f"{prefix}_{img_path.stem}.txt")

                if class_remap:
                    # Remap class IDs
                    lines = label_path.read_text().strip().split("\n")
                    remapped = []
                    for line in lines:
                        if not line.strip():
                            continue
                        parts = line.strip().split()
                        old_cls = int(parts[0])
                        new_cls = class_remap.get(str(old_cls), old_cls)
                        remapped.append(f"{new_cls} {' '.join(parts[1:])}")
                    dst_label.write_text("\n".join(remapped), encoding="utf-8")
                else:
                    shutil.copy2(label_path, dst_label)

            count += 1

    logger.info("Merged %d images from %s", count, roboflow_dir.name)
    return count


def generate_synthetic_labels(
    output_dir: Path,
    num_per_split: dict[str, int] | None = None,
) -> int:
    """Generate synthetic labeled training data for development/testing.

    Creates solid-color images with random YOLO annotations.
    This is for pipeline testing ONLY — real training needs real data.
    """
    import random

    import numpy as np

    if num_per_split is None:
        num_per_split = {"train": 100, "val": 20, "test": 10}

    count = 0
    random.seed(42)
    np.random.seed(42)

    for split, num in num_per_split.items():
        for i in range(num):
            # Create a synthetic "track" image (640x480, dark gray with lines)
            img = np.full((480, 640, 3), 80, dtype=np.uint8)
            # Add rail-like lines
            import cv2

            cv2.line(img, (200, 0), (200, 480), (120, 120, 120), 3)
            cv2.line(img, (440, 0), (440, 480), (120, 120, 120), 3)
            # Add some texture
            noise = np.random.randint(0, 20, img.shape, dtype=np.uint8)
            img = cv2.add(img, noise)

            # Save image
            img_path = output_dir / split / "images" / f"synth_{i:04d}.jpg"
            cv2.imwrite(str(img_path), img)

            # Generate 0-3 random annotations
            num_defects = random.randint(0, 3)
            labels = []
            for _ in range(num_defects):
                cls_id = random.randint(0, len(CLASSES) - 1)
                cx = random.uniform(0.1, 0.9)
                cy = random.uniform(0.1, 0.9)
                w = random.uniform(0.02, 0.15)
                h = random.uniform(0.02, 0.15)
                labels.append(f"{cls_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}")

            label_path = output_dir / split / "labels" / f"synth_{i:04d}.txt"
            label_path.write_text("\n".join(labels), encoding="utf-8")
            count += 1

    logger.info("Generated %d synthetic samples", count)
    return count


def main():
    parser = argparse.ArgumentParser(description="Prepare RailScan training dataset")
    parser.add_argument(
        "--output", type=Path, default=Path("./datasets/railscan"),
        help="Output directory for the merged dataset",
    )
    parser.add_argument(
        "--roboflow-key", type=str, default=None,
        help="Roboflow API key (or set ROBOFLOW_API_KEY env var)",
    )
    parser.add_argument(
        "--synthetic-only", action="store_true",
        help="Generate synthetic data only (for pipeline testing)",
    )
    args = parser.parse_args()

    output_dir = args.output.resolve()
    logger.info("Preparing dataset at %s", output_dir)

    # Setup directory structure
    setup_directory_structure(output_dir)

    if args.synthetic_only:
        # Quick synthetic data for development
        generate_synthetic_labels(output_dir)
    else:
        # Try to download real datasets
        import os
        api_key = args.roboflow_key or os.environ.get("ROBOFLOW_API_KEY")

        if api_key:
            # Download from Roboflow (these are known good datasets)
            datasets_to_download = [
                # (workspace, project, version) — ordered by size/quality
                ("defect-rexb3", "rail-defect", 1),           # ~4900 images, 6 classes (BEST)
                ("kakashi-fmrs2", "rail-surface-defects-d9i8o", 1),  # 9 classes
                ("thesis-group", "railway-crack-detection", 1),       # 936 images
                ("raildefectunal", "rail-defects-detection", 2),      # 207 images, 6 pro classes
            ]

            for ws, proj, ver in datasets_to_download:
                try:
                    rf_dir = download_roboflow_dataset(ws, proj, ver, api_key, output_dir)
                    if rf_dir:
                        merge_roboflow_data(rf_dir, output_dir, prefix=f"rf_{proj[:8]}")
                except Exception as e:
                    logger.warning("Failed to download %s/%s: %s", ws, proj, e)
        else:
            logger.warning(
                "No ROBOFLOW_API_KEY set. Generating synthetic data instead.\n"
                "For real training, set ROBOFLOW_API_KEY and re-run.\n"
                "Get your API key at: https://app.roboflow.com/settings/api"
            )
            generate_synthetic_labels(output_dir)

    # Create data.yaml
    create_dataset_yaml(output_dir)

    # Print summary
    for split in ("train", "val", "test"):
        img_dir = output_dir / split / "images"
        if img_dir.exists():
            count = len(list(img_dir.iterdir()))
            logger.info("  %s: %d images", split, count)

    logger.info("Dataset preparation complete!")
    logger.info("To train: python scripts/train_defect_model.py --data %s/data.yaml", output_dir)


if __name__ == "__main__":
    main()
